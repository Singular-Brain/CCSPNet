{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CCSPNet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Singular-Brain/CCSPNet/blob/main/CCSPNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLqHe2QUdFtb"
      },
      "source": [
        "#Preparing notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfKf4DRMF2cN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0bd5012-a0ae-48c4-bc19-a3564bbbd0d2"
      },
      "source": [
        "import sys\n",
        "import copy\n",
        "import time\n",
        "import math\n",
        "import random\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "from numpy import linalg as la\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import scipy\n",
        "from scipy import io, signal, fftpack\n",
        "import scipy.linalg as la\n",
        "import gzip\n",
        "import pickle\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "###Pytorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchsummary import summary\n",
        "from torchvision import transforms\n",
        "import matplotlib\n",
        "try:\n",
        "    from termcolor import colored\n",
        "    import wget\n",
        "    import mat73\n",
        "except:\n",
        "    !pip install mat73\n",
        "    !pip install termcolor\n",
        "    !pip install wget\n",
        "    import mat73\n",
        "    from termcolor import colored\n",
        "    import wget\n",
        "\n",
        "#\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mat73\n",
            "  Downloading mat73-0.55-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from mat73) (3.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mat73) (1.19.5)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->mat73) (1.5.2)\n",
            "Installing collected packages: mat73\n",
            "Successfully installed mat73-0.55\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (1.1.0)\n",
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9672 sha256=347f9eefd8223e7adf88f395ad151e48f3320d3d2d7a89af19a825c878e430f8\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgkQ4Lcj_Y64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6529ca8d-7bc6-477f-9e7e-dc70f38d3da8"
      },
      "source": [
        "#set manual seed for preprocessing\n",
        "def manual_seed(seed):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    from torch.backends import cudnn\n",
        "    cudnn.deterministic = True #type: ignore\n",
        "    cudnn.benchmark = False # type: ignore\n",
        "# We set the seed to 2045 because the Singularity is near!\n",
        "# manual_seed(2045)\n",
        "\n",
        "if (torch.cuda.is_available()):\n",
        "    device = 'cuda'\n",
        "    workers = 2\n",
        "else:\n",
        "    device = 'cpu'\n",
        "    workers = 0\n",
        "print(f'Device is set to {device}\\nNumber of workers: {workers}')\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "except:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device is set to cuda\n",
            "Number of workers: 2\n",
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIGxkklzp7Zq"
      },
      "source": [
        "# Importing data and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EC1Ovav6rzT"
      },
      "source": [
        "class EEG_MI_dataset(Dataset):\n",
        "    def __init__(self, data, label, subject, mode):\n",
        "        if mode != 'Stability':\n",
        "            subject_idx  = int(subject) - 1\n",
        "            subject = f'{int(subject):02d}'\n",
        "        else:\n",
        "            subjs = copy.deepcopy(subject)\n",
        "            subjs.append(0)\n",
        "            subject = f'{int(0):02d}'\n",
        "            subject_idx=0\n",
        "        self.section = \"train\"\n",
        "        all_subjects = [str(format(i, '02d')) for i in range(1,55)]\n",
        "        if mode == 'SI':\n",
        "            ## Data\n",
        "\n",
        "            ## 5300\n",
        "            # self.data_tot = np.delete(data[:,3:,...], obj = subject_idx, axis = 0)\n",
        "            # self.data_tot = self.data_tot.reshape((-1,1) +  self.data_tot.shape[3:])\n",
        "            # self.labels_tot = np.delete(label[:,3:,...], obj = subject_idx, axis = 0)\n",
        "            # self.labels_tot = self.labels_tot.reshape((-1,1) +  self.labels_tot.shape[3:])\n",
        "            # self.labels_tot = np.ravel(self.labels_tot)\n",
        "            # self.data_train,  self.labels_train = self.data_tot, self.labels_tot\n",
        "            # self.data_test = data[subject_idx, 3,:,np.newaxis,...]\n",
        "            # self.labels_test = np.ravel(label[subject_idx, 3,:])\n",
        "\n",
        "            ## 10600 Online\n",
        "            self.data_tot = np.delete(data[:,[1,3],...], obj = subject_idx, axis = 0)\n",
        "            self.data_tot = self.data_tot.reshape((-1,1) +  self.data_tot.shape[3:])\n",
        "            self.labels_tot = np.delete(label[:,[1,3],...], obj = subject_idx, axis = 0)\n",
        "            self.labels_tot = self.labels_tot.reshape((-1,1) +  self.labels_tot.shape[3:])\n",
        "            self.labels_tot = np.ravel(self.labels_tot)\n",
        "            self.data_train,  self.labels_train = self.data_tot, self.labels_tot\n",
        "            self.data_test = data[subject_idx, 3,:,np.newaxis,...]\n",
        "            self.labels_test = np.ravel(label[subject_idx, 3,:])\n",
        "            print(self.data_train.shape)\n",
        "\n",
        "            ## 10600 Offline\n",
        "            # self.data_tot = np.delete(data[:,[0,2],...], obj = subject_idx, axis = 0)\n",
        "            # self.data_tot = self.data_tot.reshape((-1,1) +  self.data_tot.shape[3:])\n",
        "            # self.labels_tot = np.delete(label[:,[0,2],...], obj = subject_idx, axis = 0)\n",
        "            # self.labels_tot = self.labels_tot.reshape((-1,1) +  self.labels_tot.shape[3:])\n",
        "            # self.labels_tot = np.ravel(self.labels_tot)\n",
        "            # self.data_train,  self.labels_train = self.data_tot, self.labels_tot\n",
        "            # self.data_test = data[subject_idx, 3,:,np.newaxis,...]\n",
        "            # self.labels_test = np.ravel(label[subject_idx, 3,:])\n",
        "            # print(self.data_train.shape)\n",
        "\n",
        "            ## All Data\n",
        "            # self.data_tot = np.delete(data[:,:,...], obj = subject_idx, axis = 0)\n",
        "            # self.data_tot = self.data_tot.reshape((-1,1) +  self.data_tot.shape[3:])\n",
        "            # self.labels_tot = np.delete(label[:,:,...], obj = subject_idx, axis = 0)\n",
        "            # self.labels_tot = self.labels_tot.reshape((-1,1) +  self.labels_tot.shape[3:])\n",
        "            # self.labels_tot = np.ravel(self.labels_tot)\n",
        "            # self.data_train,  self.labels_train = self.data_tot, self.labels_tot\n",
        "            # self.data_test = data[subject_idx, 3,:,np.newaxis,...]\n",
        "            # self.labels_test = np.ravel(label[subject_idx, 3,:])\n",
        "            # print(self.data_train.shape)\n",
        "\n",
        "        elif mode == 'SD':\n",
        "            self.val_idx = np.random.randint(0,200,40)\n",
        "            ## Data\n",
        "            self.data_tot = data[subject_idx,[0,1,2],...]\n",
        "            self.data_tot = self.data_tot.reshape((-1,1) +  self.data_tot.shape[2:])\n",
        "            self.label_tot = label[subject_idx,[0,1,2],...]\n",
        "            self.label_tot = self.label_tot.reshape((-1,1) +  self.label_tot.shape[2:])\n",
        "            self.label_tot = np.ravel(self.label_tot)\n",
        "\n",
        "            self.data_train, self.data_val, self.labels_train, self.labels_val = train_test_split(self.data_tot, self.label_tot, test_size=0.2, random_state=2045, shuffle=True)\n",
        "            self.data_test = data[subject_idx, 3,:,np.newaxis,...]\n",
        "            self.labels_test = np.ravel(label[subject_idx, 3,:])\n",
        "\n",
        "        ## For robustness tests\n",
        "        elif mode == 'Stability':\n",
        "            ## Data\n",
        "\n",
        "            ## 5300\n",
        "            # self.data_tot = np.delete(data[:,3:,...], obj = subject_idx, axis = 0)\n",
        "            # self.data_tot = self.data_tot.reshape((-1,1) +  self.data_tot.shape[3:])\n",
        "            # self.labels_tot = np.delete(label[:,3:,...], obj = subject_idx, axis = 0)\n",
        "            # self.labels_tot = self.labels_tot.reshape((-1,1) +  self.labels_tot.shape[3:])\n",
        "            # self.labels_tot = np.ravel(self.labels_tot)\n",
        "            # self.data_train,  self.labels_train = self.data_tot, self.labels_tot\n",
        "            # self.data_test = data[subject_idx, 3,:,np.newaxis,...]\n",
        "            # self.labels_test = np.ravel(label[subject_idx, 3,:])\n",
        "\n",
        "            ## 10600 Online\n",
        "            # data = data[subjs,...]\n",
        "            # self.data_tot = np.delete(data[:,[1,3],...], obj = 0, axis = 0)\n",
        "            # self.data_tot = self.data_tot.reshape((-1,1) +  self.data_tot.shape[3:])\n",
        "            # label = label[subjs,...]\n",
        "            # self.labels_tot = np.delete(label[:,[1,3],...], obj = 0, axis = 0)\n",
        "            # self.labels_tot = self.labels_tot.reshape((-1,1) +  self.labels_tot.shape[3:])\n",
        "            # self.labels_tot = np.ravel(self.labels_tot)\n",
        "            # self.data_train,  self.labels_train = self.data_tot, self.labels_tot\n",
        "            # self.data_test = data[subject_idx, 3,:,np.newaxis,...]\n",
        "            # self.labels_test = np.ravel(label[subject_idx, 3,:])\n",
        "            # print(self.data_train.shape)\n",
        "\n",
        "            ## 10600 Offline\n",
        "            data = data[subjs,...]\n",
        "            self.data_tot = np.delete(data[:,[0,2],...], obj = subject_idx, axis = 0)\n",
        "            self.data_tot = self.data_tot.reshape((-1,1) +  self.data_tot.shape[3:])\n",
        "            label = label[subjs,...]\n",
        "            self.labels_tot = np.delete(label[:,[0,2],...], obj = subject_idx, axis = 0)\n",
        "            self.labels_tot = self.labels_tot.reshape((-1,1) +  self.labels_tot.shape[3:])\n",
        "            self.labels_tot = np.ravel(self.labels_tot)\n",
        "            self.data_train,  self.labels_train = self.data_tot, self.labels_tot\n",
        "            self.data_test = data[subject_idx, 3,:,np.newaxis,...]\n",
        "            self.labels_test = np.ravel(label[subject_idx, 3,:])\n",
        "            print(self.data_train.shape)\n",
        "\n",
        "            ## All Data\n",
        "            # self.data_tot = np.delete(data[:,:,...], obj = subject_idx, axis = 0)\n",
        "            # self.data_tot = self.data_tot.reshape((-1,1) +  self.data_tot.shape[3:])\n",
        "            # self.labels_tot = np.delete(label[:,:,...], obj = subject_idx, axis = 0)\n",
        "            # self.labels_tot = self.labels_tot.reshape((-1,1) +  self.labels_tot.shape[3:])\n",
        "            # self.labels_tot = np.ravel(self.labels_tot)\n",
        "            # self.data_train,  self.labels_train = self.data_tot, self.labels_tot\n",
        "            # self.data_test = data[subject_idx, 3,:,np.newaxis,...]\n",
        "            # self.labels_test = np.ravel(label[subject_idx, 3,:])\n",
        "            # print(self.data_train.shape)        \n",
        "               \n",
        "        ### Emptying Ram\n",
        "        for element in [data, label]:\n",
        "            del(element)\n",
        "        ### Copying arrays\n",
        "        self.data_train, self.labels_train  = self.data_train.copy(), self.labels_train.copy()\n",
        "        self.data_test,  self.labels_test   = self.data_test.copy(),  self.labels_test.copy()\n",
        "\n",
        "    def train(self):\n",
        "        self.section = \"train\"\n",
        "\n",
        "    def val(self):\n",
        "        self.section = \"val\"\n",
        "\n",
        "    def test(self):\n",
        "        self.section = \"test\"\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.section == 'test':\n",
        "            return len(self.labels_test)\n",
        "        elif self.section == 'val':\n",
        "            return len(self.labels_val)\n",
        "        elif self.section == 'train':\n",
        "            return len(self.labels_train)\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.section == 'test':\n",
        "            return self.data_test[idx], self.labels_test[idx]\n",
        "        elif self.section == 'val':\n",
        "            return self.data_val[idx], self.labels_val[idx]\n",
        "        elif self.section == 'train':\n",
        "            return self.data_train[idx], self.labels_train[idx]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1EdGcoJfAYR"
      },
      "source": [
        "\n",
        "# Load data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIuHkoHWKJKH"
      },
      "source": [
        "\n",
        "\n",
        "Link to the preprocessed data and label:\n",
        "\n",
        "https://drive.google.com/drive/folders/1pcskugvKgo5sCuFzAJbiK6xNO1up12JO?usp=sharing\n",
        "\n",
        "Create a shortcut of the shared folder in your Drive, and run the following cell to load the data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYjLE_FMa1Nj"
      },
      "source": [
        "data    =  mat73.loadmat('/content/drive/My Drive/CCSPNet_data_preprocessed/Preprocessed_Data.mat')['Data']\n",
        "data    =  np.moveaxis(data , -1, -2)\n",
        "labels  =  io.loadmat('/content/drive/My Drive/CCSPNet_data_preprocessed/Labels.mat')['Labels']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wu65V7GqH_ML"
      },
      "source": [
        "# NETWORK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dj6NTijsipmy"
      },
      "source": [
        "class CCSP(nn.Module):\n",
        "    def __init__(self, kernLength, timepoints, nb_classes = 2 ,wavelet_filters = 4, wavelet_kernel = 64 ,nn_layers= [4],feature_reduction_network = [4], nchans = 62, n_CSP_subspace_vectors = 1 ,):\n",
        "        super(CCSP, self).__init__()\n",
        "        #manual_seed(2045)\n",
        "        self.T = timepoints     \n",
        "        self.m = n_CSP_subspace_vectors\n",
        "        self.F = nn_layers[-1] if nn_layers else wavelet_filters\n",
        "        self.CSP = CommonSpatialPattern(self.F, self.m, nchans)\n",
        "        self.register_parameter(name='CSP projection matrix', param= self.CSP.W)\n",
        "        self.CSP_fig = None\n",
        "        self.kernLength = kernLength\n",
        "        self.wavelet_filters = wavelet_filters\n",
        "        self.wavelet_kernel = wavelet_kernel\n",
        "        self.nn_layers = nn_layers\n",
        "        self.batch_norm = nn.BatchNorm1d(2* self.m * self.F)\n",
        "        self.feature_reduction_network = feature_reduction_network\n",
        "        ### Wawelet\n",
        "        self.wavelet_padding = nn.ZeroPad2d((int(wavelet_kernel/2)-1, int(wavelet_kernel/2), 0, 0))\n",
        "        self.freq = torch.tensor([[4 + (i+1) * 36/(self.wavelet_filters + 1)] for i in range(self.wavelet_filters)], requires_grad= True, device= device)\n",
        "        self.fwhm = torch.tensor([[.1] for _ in range(self.wavelet_filters)], requires_grad= True, device= device)\n",
        "        self.coefficient = torch.tensor([[4* math.log(2)] for _ in range(self.wavelet_filters)], requires_grad= True, device= device)\n",
        "        ### Neural Network\n",
        "        self.CNN = nn.ModuleList()\n",
        "        prev_chans = wavelet_filters if wavelet_filters else 1\n",
        "        for i, chans in enumerate(nn_layers):\n",
        "            if i == len(nn_layers) -1:\n",
        "                last_block= True\n",
        "            else:\n",
        "                last_block=False\n",
        "            self.CNN.append(self.CNN_block(prev_chans, chans, last_block))\n",
        "            prev_chans = chans\n",
        "        ### FRN\n",
        "        self.FRN = nn.ModuleList()\n",
        "        prev_size = self.F * 2 * self.m\n",
        "        for i, size in enumerate(feature_reduction_network):\n",
        "            if i == len(feature_reduction_network) -1:\n",
        "                last_block= True\n",
        "            else:\n",
        "                last_block=False\n",
        "            self.FRN.append(self.FRN_block(prev_size, size, last_block)) \n",
        "            prev_size = size\n",
        "        ### LDA\n",
        "        if self.FRN:\n",
        "            self.LDA = LinearDiscriminantAnalysis(feature_reduction_network[-1])\n",
        "        else:\n",
        "            self.LDA = LinearDiscriminantAnalysis(2 * self.F * self.m)\n",
        "        self.register_parameter(name='LDA projection matrix', param= self.LDA.W)\n",
        "\n",
        "    def plot_CSP(self, CSP_output, labels):\n",
        "        print('CSP Plots:')\n",
        "        label = labels.cpu()\n",
        "        rows = math.ceil(self.F/4)\n",
        "        self.CSP_fig = plt.figure(figsize = (16,4 * rows))\n",
        "        for i in range(self.F):\n",
        "            points = CSP_output[:,i,...].squeeze(-1).cpu().detach().numpy()\n",
        "            class0 = points[label==0]\n",
        "            class1 = points[label==1]\n",
        "            ax = plt.subplot(rows, 4, i+1)\n",
        "            ax.plot(class0[:,0], class0[:,-1], 'ob')\n",
        "            ax.plot(class1[:,0], class1[:,-1], 'or', alpha = .6)\n",
        "            ax.set_yticklabels([])\n",
        "            ax.set_xticklabels([])\n",
        "        plt.show()\n",
        "\n",
        "    def _design_wavelet(self, kernLength, freq, fwhm, coefficient, fs = 100):\n",
        "            timevec = torch.arange(kernLength) / fs\n",
        "            timevec = timevec - torch.mean(timevec)\n",
        "            timevec = timevec.repeat(self.wavelet_filters).reshape(self.wavelet_filters, kernLength).to(device)\n",
        "            csw = torch.cos(2*math.pi*freq*timevec)\n",
        "            gus = torch.exp(-(coefficient * torch.pow(timevec, 2)/torch.pow(fwhm,2)))\n",
        "            return (csw * gus).unsqueeze(1).unsqueeze(1)\n",
        "\n",
        "    def CNN_block(self, prev_chans, chan, last_block):\n",
        "        if not last_block:\n",
        "            return nn.Sequential(\n",
        "                nn.ZeroPad2d((int(self.kernLength/2)-1, int(self.kernLength/2), 0, 0)),\n",
        "                nn.Conv2d(prev_chans, chan, (1, self.kernLength), padding =0, bias = False),\n",
        "                nn.Sigmoid(),\n",
        "                nn.BatchNorm2d(chan),\n",
        "                )\n",
        "        else:\n",
        "            return nn.Sequential(\n",
        "                nn.ZeroPad2d((int(self.kernLength/2)-1, int(self.kernLength/2), 0, 0)),\n",
        "                nn.Conv2d(prev_chans, chan, (1, self.kernLength), padding =0, bias = False),\n",
        "                )\n",
        "\n",
        "    \n",
        "    def FRN_block(self, prev_size, size, last_block):\n",
        "        if not last_block:\n",
        "            return nn.Sequential(\n",
        "                nn.Linear(prev_size, size),\n",
        "                nn.Sigmoid(),\n",
        "                nn.BatchNorm1d(size),\n",
        "                )\n",
        "        else:\n",
        "            return nn.Sequential(\n",
        "                nn.Linear(prev_size, size),\n",
        "            )\n",
        "\n",
        "    def copy(self):\n",
        "        state = {}\n",
        "        state['wavelet'] = (copy.deepcopy(self.freq),\n",
        "                            copy.deepcopy(self.fwhm), \n",
        "                            copy.deepcopy(self.coefficient))\n",
        "        state['state dict'] = copy.deepcopy(self.state_dict())\n",
        "        state['CSP'] = copy.deepcopy((self.CSP.W.data).clone().detach().tolist())\n",
        "        state['LDA'] = copy.deepcopy((self.LDA.W.data).clone().detach().tolist())\n",
        "        return state\n",
        "\n",
        "    def paste(self, state):\n",
        "        self.freq, self.fwhm, self.coefficient = state['wavelet']\n",
        "        self.load_state_dict(state['state dict'])\n",
        "        self.CSP.W.data = torch.tensor(state['CSP'], device = device) \n",
        "        self.LDA.W.data = torch.tensor(state['LDA'], device = device)\n",
        "\n",
        "    def fit_modules(self, X, y):\n",
        "        _, data = self(X)\n",
        "        self.CSP.fit(data['CNN output'], y)\n",
        "        self.LDA.fit(data['LDA input'], y)\n",
        "\n",
        "    def pred(self, X, y):\n",
        "        training_state = self.training\n",
        "        self.training = False\n",
        "        Y, _ = self(X)\n",
        "        MU = torch.tensor([torch.mean(Y[y == 0]),torch.mean(Y[y==1])]).reshape(-1,1).to(device)\n",
        "        yhat = torch.zeros(X.shape[0])\n",
        "        for i, xi in enumerate(Y):\n",
        "            dis0 = torch.sqrt(torch.dot(((xi-MU[0]).T),(xi-MU[0])))\n",
        "            dis1 = torch.sqrt(torch.dot(((xi-MU[1]).T),(xi-MU[1])))\n",
        "            if dis0 <= dis1:\n",
        "                yhat[i] = 0\n",
        "            else:\n",
        "                yhat[i] = 1\n",
        "        self.training = training_state\n",
        "        return yhat.to(device)\n",
        "\n",
        "\n",
        "    def forward(self, x, visualization = False):\n",
        "        \"\"\"\n",
        "        x:   input - shape: [N, 1, NEc, Tp]\n",
        "        N:   Batch size\n",
        "        NEc: Number of EEG channels\n",
        "        Tp:  Time point\n",
        "        \"\"\"\n",
        "        data = {}\n",
        "        ### Wavelet:\n",
        "        if self.wavelet_filters:\n",
        "            x = self.wavelet_padding(x)\n",
        "            self.wavelet_weight = self._design_wavelet(self.wavelet_kernel, self.freq, self.fwhm, self.coefficient ) \n",
        "            x = F.conv2d(x, weight = self.wavelet_weight,bias = None)   # [N, F1, NEc, Tp]\n",
        "        ### Network:\n",
        "        for layer in self.CNN:\n",
        "            x = layer(x)                        \n",
        "        CNN_output =  x                                                 # [N, F1, NEc, Tp]\n",
        "        data['CNN output'] = CNN_output\n",
        "        ### CSP\n",
        "        Y = self.CSP(CNN_output)                                        # [N, F1, 2m, Tp]\n",
        "        CSP_output = torch.log(torch.var(Y, axis = -1))                 # [N, F1, 2m]\n",
        "        if visualization:\n",
        "            self.plot_CSP(CSP_output)\n",
        "        data[\"CSP output\"]= CSP_output\n",
        "        LDA_input = CSP_output.reshape((-1, 2 * self.m * self.F))       # [N, F1*2m]\n",
        "        LDA_input = self.batch_norm(LDA_input)\n",
        "        ### Feature Reduction Network\n",
        "        if self.FRN:\n",
        "            x = LDA_input\n",
        "            for layer in self.FRN:\n",
        "                x = layer(x)\n",
        "            LDA_input = x                                               # [N, last NN layer size]\n",
        "        ### LDA\n",
        "        output = self.LDA(LDA_input)                                    # [N, 1]\n",
        "        data['LDA input'] = LDA_input\n",
        "        return output, data           "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aa7fSRudQXTR"
      },
      "source": [
        "##CSP & LDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r36llDdI2cjl"
      },
      "source": [
        "def cov(m, y=None):\n",
        "    if y is not None:\n",
        "        m = torch.cat((m, y), dim=0)\n",
        "    m_exp = torch.mean(m, dim=1)\n",
        "    x = m - m_exp[:, None]\n",
        "    cov = 1 / (x.size(1) - 1) * x@(x.T)\n",
        "    return cov\n",
        "\n",
        "#### CSP\n",
        "class CommonSpatialPattern(nn.Module):\n",
        "    def __init__(self, n_temporal_filters, n_subspace_vectors, NEc):\n",
        "        super().__init__()\n",
        "        self.F = n_temporal_filters\n",
        "        self.m = n_subspace_vectors\n",
        "        self.W = Parameter(torch.ones((self.F ,2* self.m, NEc), device = device))\n",
        "\n",
        "    def forward(self, X):\n",
        "        return self.W @ X\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        if type(y) != np.ndarray:\n",
        "            y = y.cpu().detach().numpy()\n",
        "        unique_values = np.unique(y, axis = 0)\n",
        "        class0 = X[[True if (i==unique_values[0]).all() else False for i in y]] # (N_class 0 , F, NEc, Tp)\n",
        "        class1 = X[[True if (i==unique_values[1]).all() else False for i in y]] # (N_class 1 , F, NEc, Tp)\n",
        "        total_W =  torch.tensor([])                                             # (F, 2*m , NEc)\n",
        "        for i in range(self.F):\n",
        "            class0_i = class0[:,i,...]                                          # (N_class 1 , 1, NEc, Tp)\n",
        "            class1_i = class1[:,i,...]\n",
        "            class0_i = class0_i - class0_i.mean(axis = 2).reshape(class0_i.shape[0], class0_i.shape[1],1)\n",
        "            class1_i = class1_i - class1_i.mean(axis = 2).reshape(class1_i.shape[0], class1_i.shape[1],1)\n",
        "            RH = 0\n",
        "            for x in class1_i:\n",
        "                RH += ((x@x.T)/(torch.trace(x@x.T)+ 1E-6))\n",
        "            RH = RH / class1_i.shape[0]\n",
        "            ####\n",
        "            RL = 0\n",
        "            for x in class0_i:\n",
        "                RL += (x@x.T)/(torch.trace(x@x.T+ 1E-6))\n",
        "            RL = RL / class0_i.shape[0] \n",
        "            RL = RL.cpu().detach().numpy()\n",
        "            RH = RH.cpu().detach().numpy()\n",
        "            try:\n",
        "                v, u = la.eigh(RL, RH)\n",
        "            except Exception as e:\n",
        "                print('!', e)\n",
        "                RH += np.random.random(RH.shape) * 1E-4\n",
        "                RL += np.random.random(RL.shape) * 1E-4\n",
        "                v, u = la.eigh(RL, RH)\n",
        "            sorted_u = u[:,abs(v).argsort()[::-1]]\n",
        "            W = torch.tensor(np.concatenate((sorted_u[:,:self.m], sorted_u[:,-self.m:]), axis = 1)) #(NEc, 2m)\n",
        "            total_W = torch.cat((total_W, W.T.unsqueeze(0)), 0)                                     #(F, 2m, NEc)\n",
        "        self.W.data = total_W.to(device)                                                            #(F, 2m, NEc)\n",
        "\n",
        "\n",
        "###LDA        \n",
        "class LinearDiscriminantAnalysis(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super().__init__()\n",
        "        self.W = Parameter(torch.zeros((input_dim, 1), device = device))\n",
        "\n",
        "    def forward(self, X):\n",
        "        return (self.W.T @ X.T).T\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        if type(y) != np.ndarray:\n",
        "            y = y.cpu().detach().numpy()\n",
        "        unique_values = np.unique(y, axis = 0)\n",
        "        Xc1 = (X[[True if (i==unique_values[0]).all() else False for i in y]]).T # (N_class 0 , F, NEc, Tp)\n",
        "        Xc2 = (X[[True if (i==unique_values[1]).all() else False for i in y]]).T # (N_class 1 , F, NEc, Tp)\n",
        "\n",
        "        mu1 = Xc1.mean(axis=1).reshape(-1, 1)\n",
        "        mu2 = Xc2.mean(axis=1).reshape(-1, 1)\n",
        "        Sp1 = cov(Xc1)       #2m,  2m\n",
        "        Sp2 = cov(Xc2)\n",
        "        Sb = (mu1- mu2)@((mu1- mu2).T)\n",
        "        Sw = Sp1 + Sp2\n",
        "        Sw = Sw.cpu().detach().numpy()\n",
        "        Sb = Sb.cpu().detach().numpy()\n",
        "        try:\n",
        "            A = np.linalg.inv(Sw)@Sb\n",
        "        except Exception as e:\n",
        "            print('!', e)\n",
        "            Sw += np.random.random(Sw.shape) * 1E-4\n",
        "            A = np.linalg.inv(Sw)@Sb\n",
        "        u, v = la.eig(A)        #v: eigenvector , u: eigenvalue\n",
        "        sorted_v = v[:,np.argsort(abs(u))[::-1]]\n",
        "        ### Update W\n",
        "        self.W.data = torch.tensor(sorted_v[:, 0].reshape(-1, 1)).float().to(device) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pGLzTR3Tqux"
      },
      "source": [
        "##Utility Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0KHEWl_SvDG"
      },
      "source": [
        "def color(x):\n",
        "    if x < 60:\n",
        "        c = 'red'\n",
        "    elif x < 75:\n",
        "        c = 'yellow'\n",
        "    else:\n",
        "        c = 'green'\n",
        "    return colored(x, c)\n",
        "    \n",
        "from scipy.fft import rfft, rfftfreq\n",
        "def plot_fft(signal):\n",
        "    yf = rfft(signal)\n",
        "    xf = rfftfreq(64, 1 / 100)\n",
        "    plt.plot(xf, np.abs(yf))\n",
        "\n",
        "\n",
        "### Loss\n",
        "def LDALoss(pred, label, alpha = 0.001, epsilon = 1E-5):\n",
        "    def generator(X):\n",
        "        return 1/(X+1) - 1/2\n",
        "    pred = pred.reshape(-1).float()\n",
        "    label = label.reshape(-1).float()\n",
        "    return (label @ (pred) + (label-1) @ (pred)) / len(label)\n",
        "def MVLoss(pred, label):\n",
        "    epsilon = 1e-5\n",
        "    c0 = pred[label==0]\n",
        "    c1 = pred[label==1]\n",
        "    return (torch.var(c0) + torch.var(c1)) / ((c0.mean() - c1.mean())**2 + epsilon)\n",
        "\n",
        "def output_format(labels, m):\n",
        "    label_form = lambda label, m: [1- label] * m + [label] * m\n",
        "    return torch.tensor([label_form(label, m) for label in labels]).float().to(device)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QSIY8SpTub6"
      },
      "source": [
        "#Train and Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftYKgprHTIRZ"
      },
      "source": [
        "def train(model, dataloader, epochs, m, validation = False, loss_ratio = .5,\n",
        "          lr = 0.01, wavelet_lr = 0.1, criterion = MVLoss, lambda1 = 1e-2, lambda2=1e-1,\n",
        "          early_stop_patience = None, verbose = 2, tensorboard = False,\n",
        "          scheduler = None):\n",
        "    assert verbose in [0,1,2], \"'vebose' can be one of 0, 1 or 2 \"\n",
        "    assert 0 <= loss_ratio <= 1, \"'loss_ratio must be between 0 and 1\"\n",
        "    manual_seed(2045)\n",
        "    optimizer = optim.Adam(model.parameters(), lr =lr, weight_decay=lambda2)\n",
        "    optimizer.add_param_group({'params': model.freq, \"lr\" : wavelet_lr * 10})\n",
        "    optimizer.add_param_group({'params': model.fwhm, \"lr\" : wavelet_lr})\n",
        "    optimizer.add_param_group({'params': model.coefficient, \"lr\" : wavelet_lr})\n",
        "    # scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.2,\n",
        "    #        patience=5, min_lr=1E-5, verbose=True if verbose in [2] else False)\n",
        "    batch_size = dataloader.batch_size\n",
        "    n_batches = math.ceil(dataloader.dataset.data_train.shape[0]/batch_size)\n",
        "    if verbose in [1,2]:\n",
        "        print('Number of Trials:',dataloader.dataset.data_train.shape[0],\n",
        "              'Batch Size:', batch_size,'Number of Batches:', n_batches)\n",
        "        print('Train input shape:', dataloader.dataset.data_train.shape)\n",
        "    if tensorboard:\n",
        "        tb = SummaryWriter(f'runs/model01/{int(datetime.now().timestamp())}')\n",
        "    train_history = {\"train_loss\":[],\"train_accuracy\":[], \"model\": []}\n",
        "    training_start = time.time()\n",
        "    dataset.train()\n",
        "    all_inputs, all_labels = torch.tensor(dataloader.dataset.data_train).float().to(device), torch.tensor(dataloader.dataset.labels_train).to(device)\n",
        "    model.fit_modules(all_inputs, all_labels)\n",
        "    for epoch in range(epochs):\n",
        "        if verbose in [2, 3]:\n",
        "            print('='*70 + '\\n' + f'Epoch: {epoch + 1}/ {epochs}', end = '\\r' )\n",
        "        epoch_start = time.time()\n",
        "        running_loss = 0.0\n",
        "        if verbose in [2, 3]:\n",
        "            print(f'\\n@ Batch: {1} / {n_batches}', end = '\\r')\n",
        "        dataset.train()\n",
        "        for batch ,(inputs, labels) in enumerate(dataloader):\n",
        "            batch_start = time.time()\n",
        "            ### Wrap them in Variable\n",
        "            inputs, labels = Variable(inputs).float().to(device), Variable(labels).to(device)\n",
        "            ### Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "            ###  Forward\n",
        "            model.train()\n",
        "            outputs, data = model(inputs)\n",
        "            if False:\n",
        "                rows = math.ceil(model.wavelet_filters/4)\n",
        "                plt.figure(figsize = (16,4 * rows))\n",
        "                for i in range(model.wavelet_filters):\n",
        "                    ax = plt.subplot(rows, 4, i+1)\n",
        "                    plt.plot(model.wavelet_weight[i,0,0,:].detach().cpu())\n",
        "                    ax.set_yticklabels([])\n",
        "                    ax.set_xticklabels([])\n",
        "                plt.show()\n",
        "                plt.figure(figsize = (16,4 * rows))\n",
        "                for i in range(model.wavelet_filters):\n",
        "                    ax = plt.subplot(rows, 4, i+1)\n",
        "                    plot_fft(model.wavelet_weight[i,0,0,:].detach().cpu().numpy())\n",
        "                plt.show()\n",
        "            ### CSP Loss\n",
        "            CSP_output_softmax = F.softmax(data['CSP output'], dim = -1)\n",
        "            CSP_loss = 0\n",
        "            BCElabels = output_format(labels, m)\n",
        "            for f in range(CSP_output_softmax.size(1)):\n",
        "                CSP_loss += nn.BCELoss()(CSP_output_softmax[:,f,...].float(), BCElabels)\n",
        "            CSP_loss = CSP_loss / CSP_output_softmax.size(1)\n",
        "            ### L1 reguralization\n",
        "            all_linear1_params = torch.cat([param.view(-1) for name, param in model.named_parameters() if 'weight' in name])\n",
        "            l1_regularization_loss = lambda1 * torch.norm(all_linear1_params, 1)\n",
        "            ### Backward + Optimize\n",
        "            #formatted_labels = output_format(labels, outputs)\n",
        "            loss = loss_ratio * CSP_loss + (1- loss_ratio) * criterion(outputs, labels.type(torch.LongTensor).to(device)) + l1_regularization_loss\n",
        "            loss.backward(retain_graph=False)\n",
        "            optimizer.step()\n",
        "            ### Calculate batch loss and accuracy\n",
        "            running_loss += loss.item()\n",
        "            predicted_labels = model.pred(inputs, labels)\n",
        "            batch_accuracy = (predicted_labels == labels).sum().item()/(len(inputs))\n",
        "            batch_end = time.time()\n",
        "            if verbose in [2, 3]:\n",
        "                print(f'\\033[K\\r@ Batch: {batch + 1} / {n_batches} | Batch Loss: { loss.item():.4f}  |  Batch Accuracy: {batch_accuracy * 100:.2f} | Batch Duration: {time.strftime(\"%H:%M:%S\", time.gmtime(batch_end - batch_start))}', end = '')\n",
        "        if verbose in [2, 3]:\n",
        "            print('\\nCalculating Accuracies...', end = '\\r')\n",
        "        ### Validation\n",
        "        with torch.no_grad():\n",
        "            ################ Update LDA & CSP\n",
        "            if batch_size < len(dataloader.dataset.data_train):\n",
        "                if verbose in [3]:\n",
        "                    ### Evaluate model loss and accuracy before updateing CSP & LDA\n",
        "                    model.eval()\n",
        "                    ts_corrects = 0\n",
        "                    dataset.val()\n",
        "                    for batch ,(test_inputs, test_labels) in enumerate(dataloader):\n",
        "                        test_inputs, test_labels = Variable(test_inputs).float().to(device), Variable(test_labels).to(device)\n",
        "                        test_outputs, test_data =  model(test_inputs)\n",
        "                        CSP_output_softmax = F.softmax(test_data['CSP output'], dim = -1)\n",
        "                        CSP_loss = 0\n",
        "                        BCElabels = output_format(test_labels, m)\n",
        "                        for f in range(CSP_output_softmax.size(1)):\n",
        "                            CSP_loss += nn.BCELoss()(CSP_output_softmax[:,f,...].float(), BCElabels)\n",
        "                        CSP_loss = CSP_loss / CSP_output_softmax.size(1)\n",
        "                        #formatted_labels = output_format(test_labels, test_outputs)\n",
        "                        test_loss = loss_ratio * CSP_loss + (1 - loss_ratio) * criterion(test_outputs, labels.type(torch.LongTensor).to(device))\n",
        "                        predicted_labels = model.pred(test_inputs, test_labels).to(device)\n",
        "                        ts_corrects += (predicted_labels == test_labels).sum().item()\n",
        "                        ts_acc =  (predicted_labels == test_labels).sum().item()/(len(test_inputs))\n",
        "                        print(f'\\033[Kaccuracy before updating CSP and LDA | @ Batch: {batch + 1} / | Acc = {ts_acc}', end = '\\r')\n",
        "                    total_ts_acc = (ts_corrects/len(dataloader.dataset))\n",
        "                    print(f'\\033[Kaccuracy before updating CSP and LDA: {total_ts_acc:.4f}                                ')\n",
        "                    \n",
        "                ### forward pass: Update the projection matrixes of CSP and LDA\n",
        "                if verbose in [2, 3]:\n",
        "                    print('\\033[KForward pass', end = '\\r')\n",
        "                model.fit_modules(all_inputs, all_labels)\n",
        "                ### Calculate new train loss and accuracy\n",
        "                if verbose in [2, 3]:\n",
        "                    print('\\rCalculating new Train Loss and Accuracy', end = '\\r')\n",
        "                predicted_labels = model.pred(inputs, labels)\n",
        "                total_tr_acc = (predicted_labels == labels).sum().item()/(len(inputs))\n",
        "                total_tr_loss = loss\n",
        "            else:\n",
        "                total_tr_acc = batch_accuracy\n",
        "                total_tr_loss = loss\n",
        "\n",
        "            ######################\n",
        "            ### Calculate test Loss & Accuracy\n",
        "            if validation:\n",
        "                if verbose in [2]:\n",
        "                    print('\\rCalculating test Loss & Accuracy', end = '\\r')\n",
        "                model.eval()\n",
        "                ts_corrects = 0\n",
        "                dataset.val()\n",
        "                for batch ,(test_inputs, test_labels) in enumerate(dataloader):\n",
        "                    test_inputs, test_labels = Variable(test_inputs).float().to(device), Variable(test_labels).to(device)\n",
        "                    test_outputs, test_data =  model(test_inputs)\n",
        "                    CSP_output_softmax = F.softmax(test_data['CSP output'], dim = -1)\n",
        "                    CSP_loss = 0\n",
        "                    BCElabels = output_format(test_labels, m)\n",
        "                    for f in range(CSP_output_softmax.size(1)):\n",
        "                        CSP_loss += nn.BCELoss()(CSP_output_softmax[:,f,...].float(), BCElabels)\n",
        "                        # print('*', f, ':',BCE.sum(axis = -1).mean().item())\n",
        "                    CSP_loss = CSP_loss / CSP_output_softmax.size(1)  \n",
        "                    test_loss = loss_ratio * CSP_loss + (1 - loss_ratio) * criterion(test_outputs, test_labels.type(torch.LongTensor).to(device))\n",
        "                    # print('TEST: LDA Loss', criterion(test_outputs, test_labels) ,' , CSP Loss', CSP_loss)\n",
        "                    predicted_labels = model.pred(test_inputs, test_labels).to(device)\n",
        "                    ts_corrects += (predicted_labels == test_labels).sum().item()\n",
        "                    ts_acc =  (predicted_labels == test_labels).sum().item()/(len(test_inputs))\n",
        "                    if verbose in [2]:\n",
        "                        print(f'\\033[KNew Test Accuracy | @ Batch: {batch+1} / | Acc = {ts_acc}', end = '\\r')\n",
        "                total_ts_acc = (ts_corrects/len(dataloader.dataset))\n",
        "        ### TensorBoard\n",
        "        if tensorboard:\n",
        "            if verbose in [2, 3]:\n",
        "                print('\\033[KWrite on Tensorboard...', end = '\\r')\n",
        "            for name, w in model.named_parameters():\n",
        "              if 'conv' in name:\n",
        "                tb.add_histogram(name, w, i)\n",
        "                tb.add_histogram(str(name) + ' grad', w, i)\n",
        "        ### Results\n",
        "        train_history[\"train_loss\"].append(running_loss/n_batches)\n",
        "        train_history[\"train_accuracy\"].append(total_tr_acc)\n",
        "        if validation:\n",
        "            train_history[\"val_loss\"].append(test_loss.item())\n",
        "            train_history[\"val_accuracy\"].append(total_ts_acc)\n",
        "        train_history[\"model\"].append(model.copy())\n",
        "        if early_stop_patience:\n",
        "            if epoch > early_stop_patience:\n",
        "                if train_history[\"val_loss\"][- early_stop_patience] < min( train_history[\"val_loss\"][1 - early_stop_patience:]):\n",
        "                    if verbose in [2, 3]:\n",
        "                        print(\"Early stop!\")\n",
        "                    break\n",
        "        if scheduler:\n",
        "            scheduler.step(running_loss)\n",
        "        epoch_end = time.time()\n",
        "        if verbose in [2, 3]:\n",
        "            print(f\"\\033[K ► Training Loss=  {running_loss/n_batches:.4f} |  Train Acc=  {color(round(total_tr_acc * 100,2))} | Epoch Duration: {time.strftime('%H:%M:%S', time.gmtime(epoch_end - epoch_start))}\")\n",
        "        elif verbose in [1]:\n",
        "            print(f\"\\033[K Epoch: {epoch + 1}/ {epochs}: Train Acc=  {color(round(total_tr_acc * 100,2))}\", end = '\\r')\n",
        "    training_end = time.time()\n",
        "    if verbose in [1, 2, 3]:\n",
        "        print(f\"\\nFinished training!  Training duration: {time.strftime('%H:%M:%S', time.gmtime(training_end - training_start))}\")\n",
        "    return train_history\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVCJ3c1fTJqO"
      },
      "source": [
        "def evaluate(model, state, dataloader, loss_ratio , criterion, m, verbose = 1):\n",
        "    model.paste(state)\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        ts_corrects = 0\n",
        "        test_inputs, test_labels = torch.tensor(dataloader.dataset.data_test).float().to(device), torch.tensor(dataloader.dataset.labels_test).to(device)\n",
        "        test_outputs, test_data =  model(test_inputs)\n",
        "        CSP_output_softmax = F.softmax(test_data['CSP output'], dim = -1)\n",
        "        CSP_loss = 0\n",
        "        BCElabels = output_format(test_labels, m)\n",
        "        for f in range(CSP_output_softmax.size(1)):\n",
        "            CSP_loss += nn.BCELoss()(CSP_output_softmax[:,f,...].float(), BCElabels)\n",
        "        CSP_loss = CSP_loss / CSP_output_softmax.size(1)  \n",
        "        test_loss = loss_ratio * CSP_loss + (1 - loss_ratio) * criterion(test_outputs, test_labels.type(torch.LongTensor).to(device))\n",
        "        predicted_labels = model.pred(test_inputs, test_labels).to(device)\n",
        "        ts_corrects += (predicted_labels == test_labels).sum().item()\n",
        "        ts_acc =  (predicted_labels == test_labels).sum().item()/(len(test_inputs))\n",
        "    if verbose > 0:\n",
        "        print((f\"\\033[K ► Test Loss=  {test_loss.item():.4f}  Test Acc= {color(round(ts_acc * 100, 2))}\"))\n",
        "    return test_loss, ts_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pI6WV7woMSLP"
      },
      "source": [
        "#Run Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqWpxmPUN8nF"
      },
      "source": [
        "Mode parameter: 'SD' for subject-dependent and 'SI' for subject-independent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSbbiIcXOZR7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1466ebb0-1119-486f-e92a-f4230e707692"
      },
      "source": [
        "hists_acc = []\n",
        "hists_loss = []\n",
        "manual_seed(2045)\n",
        "\n",
        "for i in range(1,55):\n",
        "    subject = i\n",
        "    print('Subject:', i)\n",
        "    dataset = EEG_MI_dataset(data, labels, subject = subject, mode = 'SD')\n",
        "    ### Define Dataloader\n",
        "    dataloader = DataLoader(dataset, batch_size = 5300, \n",
        "                            num_workers = workers, pin_memory = True,\n",
        "                            shuffle = True)\n",
        "    net_args = {\"kernLength\"                  : 32,\n",
        "                \"timepoints\"                  : 250,\n",
        "                \"wavelet_filters\"              : 4,\n",
        "                \"wavelet_kernel\"              : 64,\n",
        "                \"nn_layers\"                   : [4],\n",
        "                \"feature_reduction_network\"   : [16,8,4],\n",
        "                \"n_CSP_subspace_vectors\"      : 2,\n",
        "                \"nb_classes\"                  : 2\n",
        "    }\n",
        "\n",
        "    train_args = {\"dataloader\"      : dataloader,\n",
        "                \"epochs\"          : 20,\n",
        "                \"lr\"              : 0.01,\n",
        "                \"wavelet_lr\"      : 0.001,\n",
        "                \"loss_ratio\"      : 0.3,\n",
        "                \"criterion\"       : MVLoss,\n",
        "                \"verbose\"         : 0,\n",
        "                \"tensorboard\"     : False,\n",
        "                \"m\"               : net_args['n_CSP_subspace_vectors'],\n",
        "                'lambda1'         : 0.01,\n",
        "                'lambda2'         : 0.1\n",
        "    }\n",
        "\n",
        "    net = CCSP(**net_args).to(device)\n",
        "\n",
        "    history = train(model = net, **train_args)\n",
        "    ### Evaluate\n",
        "    best_model_state = history['model'][np.argmin(history['train_loss'])]\n",
        "    print('The Best Epoch:', np.argmin(history['train_loss']) + 1)\n",
        "    test_loss, ts_acc= evaluate(net,best_model_state, dataloader, m=net_args['n_CSP_subspace_vectors'], criterion= MVLoss,\n",
        "            loss_ratio = train_args[\"loss_ratio\"], verbose=1,)\n",
        "    hists_acc.append([test_loss, ts_acc])\n",
        "\n",
        "    best_model_state = history['model'][np.argmax(history['train_accuracy'])]\n",
        "    print('The Best Epoch based on accuracy:', np.argmax(history['train_accuracy']) + 1)\n",
        "    test_loss, ts_acc= evaluate(net,best_model_state, dataloader, m=net_args['n_CSP_subspace_vectors'], criterion= MVLoss,\n",
        "            loss_ratio = train_args[\"loss_ratio\"], verbose=1,)\n",
        "    hists_loss.append([test_loss, ts_acc])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Subject: 1\n",
            "(10600, 1, 62, 250)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:89: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at  /pytorch/aten/src/ATen/native/Copy.cpp:219.)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The Best Epoch: 20\n",
            "\u001b[K ► Test Loss=  0.8554  Test Acc= \u001b[32m84.0\u001b[0m\n",
            "The Best Epoch based on accuracy: 16\n",
            "\u001b[K ► Test Loss=  0.8635  Test Acc= \u001b[32m84.0\u001b[0m\n",
            "Subject: 2\n",
            "(10600, 1, 62, 250)\n",
            "The Best Epoch: 20\n",
            "\u001b[K ► Test Loss=  1.3491  Test Acc= \u001b[33m67.0\u001b[0m\n",
            "The Best Epoch based on accuracy: 5\n",
            "\u001b[K ► Test Loss=  1.1629  Test Acc= \u001b[33m71.0\u001b[0m\n",
            "Subject: 3\n",
            "(10600, 1, 62, 250)\n",
            "The Best Epoch: 20\n",
            "\u001b[K ► Test Loss=  0.3252  Test Acc= \u001b[32m94.0\u001b[0m\n",
            "The Best Epoch based on accuracy: 5\n",
            "\u001b[K ► Test Loss=  0.3735  Test Acc= \u001b[32m91.0\u001b[0m\n",
            "Subject: 4\n",
            "(10600, 1, 62, 250)\n",
            "The Best Epoch: 20\n",
            "\u001b[K ► Test Loss=  4.6858  Test Acc= \u001b[33m60.0\u001b[0m\n",
            "The Best Epoch based on accuracy: 15\n",
            "\u001b[K ► Test Loss=  4.6742  Test Acc= \u001b[33m60.0\u001b[0m\n",
            "Subject: 5\n",
            "(10600, 1, 62, 250)\n",
            "The Best Epoch: 20\n",
            "\u001b[K ► Test Loss=  0.4721  Test Acc= \u001b[32m89.0\u001b[0m\n",
            "The Best Epoch based on accuracy: 5\n",
            "\u001b[K ► Test Loss=  0.4853  Test Acc= \u001b[32m87.0\u001b[0m\n",
            "Subject: 6\n",
            "(10600, 1, 62, 250)\n",
            "The Best Epoch: 20\n",
            "\u001b[K ► Test Loss=  0.4909  Test Acc= \u001b[32m89.0\u001b[0m\n",
            "The Best Epoch based on accuracy: 15\n",
            "\u001b[K ► Test Loss=  0.4943  Test Acc= \u001b[32m89.0\u001b[0m\n",
            "Subject: 7\n",
            "(10600, 1, 62, 250)\n",
            "The Best Epoch: 20\n",
            "\u001b[K ► Test Loss=  7.9217  Test Acc= \u001b[33m61.0\u001b[0m\n",
            "The Best Epoch based on accuracy: 15\n",
            "\u001b[K ► Test Loss=  7.9142  Test Acc= \u001b[33m61.0\u001b[0m\n",
            "Subject: 8\n",
            "(10600, 1, 62, 250)\n",
            "The Best Epoch: 20\n",
            "\u001b[K ► Test Loss=  0.8457  Test Acc= \u001b[32m77.0\u001b[0m\n",
            "The Best Epoch based on accuracy: 15\n",
            "\u001b[K ► Test Loss=  0.8754  Test Acc= \u001b[32m76.0\u001b[0m\n",
            "Subject: 9\n",
            "(10600, 1, 62, 250)\n",
            "The Best Epoch: 20\n",
            "\u001b[K ► Test Loss=  0.7678  Test Acc= \u001b[32m78.0\u001b[0m\n",
            "The Best Epoch based on accuracy: 15\n",
            "\u001b[K ► Test Loss=  0.7675  Test Acc= \u001b[32m79.0\u001b[0m\n",
            "Subject: 10\n",
            "(10600, 1, 62, 250)\n",
            "The Best Epoch: 20\n",
            "\u001b[K ► Test Loss=  38.3425  Test Acc= \u001b[33m60.0\u001b[0m\n",
            "The Best Epoch based on accuracy: 5\n",
            "\u001b[K ► Test Loss=  22.3845  Test Acc= \u001b[33m61.0\u001b[0m\n",
            "Subject: 11\n",
            "(10600, 1, 62, 250)\n",
            "The Best Epoch: 20\n",
            "\u001b[K ► Test Loss=  25.9189  Test Acc= \u001b[31m53.0\u001b[0m\n",
            "The Best Epoch based on accuracy: 5\n",
            "\u001b[K ► Test Loss=  26.0315  Test Acc= \u001b[31m57.0\u001b[0m\n",
            "Subject: 12\n",
            "(10600, 1, 62, 250)\n",
            "The Best Epoch: 20\n",
            "\u001b[K ► Test Loss=  1403.0676  Test Acc= \u001b[31m51.0\u001b[0m\n",
            "The Best Epoch based on accuracy: 5\n",
            "\u001b[K ► Test Loss=  998.7778  Test Acc= \u001b[31m51.0\u001b[0m\n",
            "Subject: 13\n",
            "(10600, 1, 62, 250)\n",
            "The Best Epoch: 20\n",
            "\u001b[K ► Test Loss=  4.8007  Test Acc= \u001b[33m60.0\u001b[0m\n",
            "The Best Epoch based on accuracy: 5\n",
            "\u001b[K ► Test Loss=  4.9935  Test Acc= \u001b[33m65.0\u001b[0m\n",
            "Subject: 14\n",
            "(10600, 1, 62, 250)\n",
            "The Best Epoch: 20\n",
            "\u001b[K ► Test Loss=  1.7621  Test Acc= \u001b[33m72.0\u001b[0m\n",
            "The Best Epoch based on accuracy: 5\n",
            "\u001b[K ► Test Loss=  2.1525  Test Acc= \u001b[33m67.0\u001b[0m\n",
            "Subject: 15\n",
            "(10600, 1, 62, 250)\n",
            "The Best Epoch: 20\n",
            "\u001b[K ► Test Loss=  15.8144  Test Acc= \u001b[31m58.0\u001b[0m\n",
            "The Best Epoch based on accuracy: 5\n",
            "\u001b[K ► Test Loss=  9.9639  Test Acc= \u001b[33m64.0\u001b[0m\n",
            "Subject: 16\n",
            "(10600, 1, 62, 250)\n",
            "The Best Epoch: 20\n",
            "\u001b[K ► Test Loss=  2.5416  Test Acc= \u001b[33m69.0\u001b[0m\n",
            "The Best Epoch based on accuracy: 5\n",
            "\u001b[K ► Test Loss=  1.9445  Test Acc= \u001b[33m69.0\u001b[0m\n",
            "Subject: 17\n",
            "(10600, 1, 62, 250)\n",
            "The Best Epoch: 20\n",
            "\u001b[K ► Test Loss=  1.4901  Test Acc= \u001b[33m69.0\u001b[0m\n",
            "The Best Epoch based on accuracy: 5\n",
            "\u001b[K ► Test Loss=  1.4486  Test Acc= \u001b[33m70.0\u001b[0m\n",
            "Subject: 18\n",
            "(10600, 1, 62, 250)\n",
            "The Best Epoch: 20\n",
            "\u001b[K ► Test Loss=  0.2982  Test Acc= \u001b[32m94.0\u001b[0m\n",
            "The Best Epoch based on accuracy: 15\n",
            "\u001b[K ► Test Loss=  0.2949  Test Acc= \u001b[32m95.0\u001b[0m\n",
            "Subject: 19\n",
            "(10600, 1, 62, 250)\n",
            "The Best Epoch: 20\n",
            "\u001b[K ► Test Loss=  0.6396  Test Acc= \u001b[32m77.0\u001b[0m\n",
            "The Best Epoch based on accuracy: 5\n",
            "\u001b[K ► Test Loss=  0.6785  Test Acc= \u001b[32m79.0\u001b[0m\n",
            "Subject: 20\n",
            "(10600, 1, 62, 250)\n",
            "The Best Epoch: 20\n",
            "\u001b[K ► Test Loss=  0.6348  Test Acc= \u001b[32m78.0\u001b[0m\n",
            "The Best Epoch based on accuracy: 5\n",
            "\u001b[K ► Test Loss=  0.6871  Test Acc= \u001b[32m76.0\u001b[0m\n",
            "Subject: 21\n",
            "(10600, 1, 62, 250)\n",
            "The Best Epoch: 20\n",
            "\u001b[K ► Test Loss=  0.2776  Test Acc= \u001b[32m99.0\u001b[0m\n",
            "The Best Epoch based on accuracy: 5\n",
            "\u001b[K ► Test Loss=  0.2858  Test Acc= \u001b[32m99.0\u001b[0m\n",
            "Subject: 22\n",
            "(10600, 1, 62, 250)\n",
            "The Best Epoch: 20\n",
            "\u001b[K ► Test Loss=  0.3652  Test Acc= \u001b[32m94.0\u001b[0m\n",
            "The Best Epoch based on accuracy: 5\n",
            "\u001b[K ► Test Loss=  0.3523  Test Acc= \u001b[32m93.0\u001b[0m\n",
            "Subject: 23\n",
            "(10600, 1, 62, 250)\n",
            "The Best Epoch: 20\n",
            "\u001b[K ► Test Loss=  3.3401  Test Acc= \u001b[33m64.0\u001b[0m\n",
            "The Best Epoch based on accuracy: 7\n",
            "\u001b[K ► Test Loss=  3.2476  Test Acc= \u001b[33m65.0\u001b[0m\n",
            "Subject: 24\n",
            "(10600, 1, 62, 250)\n",
            "The Best Epoch: 20\n",
            "\u001b[K ► Test Loss=  62.2174  Test Acc= \u001b[31m52.0\u001b[0m\n",
            "The Best Epoch based on accuracy: 8\n",
            "\u001b[K ► Test Loss=  33.0757  Test Acc= \u001b[31m53.0\u001b[0m\n",
            "Subject: 25\n",
            "(10600, 1, 62, 250)\n",
            "The Best Epoch: 20\n",
            "\u001b[K ► Test Loss=  3.1984  Test Acc= \u001b[33m64.0\u001b[0m\n",
            "The Best Epoch based on accuracy: 8\n",
            "\u001b[K ► Test Loss=  3.5958  Test Acc= \u001b[33m64.0\u001b[0m\n",
            "Subject: 26\n",
            "(10600, 1, 62, 250)\n",
            "The Best Epoch: 20\n",
            "\u001b[K ► Test Loss=  8.0239  Test Acc= \u001b[31m58.0\u001b[0m\n",
            "The Best Epoch based on accuracy: 5\n",
            "\u001b[K ► Test Loss=  11.4410  Test Acc= \u001b[31m59.0\u001b[0m\n",
            "Subject: 27\n",
            "(10600, 1, 62, 250)\n",
            "The Best Epoch: 20\n",
            "\u001b[K ► Test Loss=  6.7044  Test Acc= \u001b[31m54.0\u001b[0m\n",
            "The Best Epoch based on accuracy: 15\n",
            "\u001b[K ► Test Loss=  6.0268  Test Acc= \u001b[31m55.0\u001b[0m\n",
            "Subject: 28\n",
            "(10600, 1, 62, 250)\n",
            "The Best Epoch: 20\n",
            "\u001b[K ► Test Loss=  0.2243  Test Acc= \u001b[32m100.0\u001b[0m\n",
            "The Best Epoch based on accuracy: 3\n",
            "\u001b[K ► Test Loss=  0.2317  Test Acc= \u001b[32m99.0\u001b[0m\n",
            "Subject: 29\n",
            "(10600, 1, 62, 250)\n",
            "The Best Epoch: 20\n",
            "\u001b[K ► Test Loss=  0.4149  Test Acc= \u001b[32m91.0\u001b[0m\n",
            "The Best Epoch based on accuracy: 15\n",
            "\u001b[K ► Test Loss=  0.3993  Test Acc= \u001b[32m91.0\u001b[0m\n",
            "Subject: 30\n",
            "(10600, 1, 62, 250)\n",
            "The Best Epoch: 20\n",
            "\u001b[K ► Test Loss=  2.1862  Test Acc= \u001b[33m66.0\u001b[0m\n",
            "The Best Epoch based on accuracy: 15\n",
            "\u001b[K ► Test Loss=  2.1944  Test Acc= \u001b[33m66.0\u001b[0m\n",
            "Subject: 31\n",
            "(10600, 1, 62, 250)\n",
            "The Best Epoch: 20\n",
            "\u001b[K ► Test Loss=  1.3146  Test Acc= \u001b[32m75.0\u001b[0m\n",
            "The Best Epoch based on accuracy: 15\n",
            "\u001b[K ► Test Loss=  1.3225  Test Acc= \u001b[32m75.0\u001b[0m\n",
            "Subject: 32\n",
            "(10600, 1, 62, 250)\n",
            "The Best Epoch: 20\n",
            "\u001b[K ► Test Loss=  0.2643  Test Acc= \u001b[32m99.0\u001b[0m\n",
            "The Best Epoch based on accuracy: 8\n",
            "\u001b[K ► Test Loss=  0.2720  Test Acc= \u001b[32m98.0\u001b[0m\n",
            "Subject: 33\n",
            "(10600, 1, 62, 250)\n",
            "The Best Epoch: 20\n",
            "\u001b[K ► Test Loss=  0.2629  Test Acc= \u001b[32m97.0\u001b[0m\n",
            "The Best Epoch based on accuracy: 15\n",
            "\u001b[K ► Test Loss=  0.2653  Test Acc= \u001b[32m97.0\u001b[0m\n",
            "Subject: 34\n",
            "(10600, 1, 62, 250)\n",
            "The Best Epoch: 20\n",
            "\u001b[K ► Test Loss=  11581.6484  Test Acc= \u001b[31m47.0\u001b[0m\n",
            "The Best Epoch based on accuracy: 8\n",
            "\u001b[K ► Test Loss=  2226.7083  Test Acc= \u001b[31m48.0\u001b[0m\n",
            "Subject: 35\n",
            "(10600, 1, 62, 250)\n",
            "The Best Epoch: 20\n",
            "\u001b[K ► Test Loss=  6.6942  Test Acc= \u001b[31m56.0\u001b[0m\n",
            "The Best Epoch based on accuracy: 7\n",
            "\u001b[K ► Test Loss=  17.7171  Test Acc= \u001b[31m53.0\u001b[0m\n",
            "Subject: 36\n",
            "(10600, 1, 62, 250)\n",
            "The Best Epoch: 20\n",
            "\u001b[K ► Test Loss=  1.0080  Test Acc= \u001b[32m82.0\u001b[0m\n",
            "The Best Epoch based on accuracy: 10\n",
            "\u001b[K ► Test Loss=  0.7615  Test Acc= \u001b[32m83.0\u001b[0m\n",
            "Subject: 37\n",
            "(10600, 1, 62, 250)\n",
            "The Best Epoch: 20\n",
            "\u001b[K ► Test Loss=  0.3198  Test Acc= \u001b[32m94.0\u001b[0m\n",
            "The Best Epoch based on accuracy: 8\n",
            "\u001b[K ► Test Loss=  0.3280  Test Acc= \u001b[32m94.0\u001b[0m\n",
            "Subject: 38\n",
            "(10600, 1, 62, 250)\n",
            "The Best Epoch: 20\n",
            "\u001b[K ► Test Loss=  40.8894  Test Acc= \u001b[31m50.0\u001b[0m\n",
            "The Best Epoch based on accuracy: 7\n",
            "\u001b[K ► Test Loss=  20.4644  Test Acc= \u001b[31m47.0\u001b[0m\n",
            "Subject: 39\n",
            "(10600, 1, 62, 250)\n",
            "The Best Epoch: 20\n",
            "\u001b[K ► Test Loss=  0.7207  Test Acc= \u001b[32m79.0\u001b[0m\n",
            "The Best Epoch based on accuracy: 8\n",
            "\u001b[K ► Test Loss=  0.7089  Test Acc= \u001b[32m83.0\u001b[0m\n",
            "Subject: 40\n",
            "(10600, 1, 62, 250)\n",
            "The Best Epoch: 20\n",
            "\u001b[K ► Test Loss=  4.1943  Test Acc= \u001b[33m62.0\u001b[0m\n",
            "The Best Epoch based on accuracy: 7\n",
            "\u001b[K ► Test Loss=  5.6927  Test Acc= \u001b[31m59.0\u001b[0m\n",
            "Subject: 41\n",
            "(10600, 1, 62, 250)\n",
            "The Best Epoch: 20\n",
            "\u001b[K ► Test Loss=  61.1739  Test Acc= \u001b[31m54.0\u001b[0m\n",
            "The Best Epoch based on accuracy: 7\n",
            "\u001b[K ► Test Loss=  40.5580  Test Acc= \u001b[31m51.0\u001b[0m\n",
            "Subject: 42\n",
            "(10600, 1, 62, 250)\n",
            "The Best Epoch: 20\n",
            "\u001b[K ► Test Loss=  0.9060  Test Acc= \u001b[32m78.0\u001b[0m\n",
            "The Best Epoch based on accuracy: 7\n",
            "\u001b[K ► Test Loss=  0.9754  Test Acc= \u001b[32m76.0\u001b[0m\n",
            "Subject: 43\n",
            "(10600, 1, 62, 250)\n",
            "The Best Epoch: 20\n",
            "\u001b[K ► Test Loss=  1.6547  Test Acc= \u001b[33m64.0\u001b[0m\n",
            "The Best Epoch based on accuracy: 7\n",
            "\u001b[K ► Test Loss=  1.7064  Test Acc= \u001b[33m64.0\u001b[0m\n",
            "Subject: 44\n",
            "(10600, 1, 62, 250)\n",
            "The Best Epoch: 20\n",
            "\u001b[K ► Test Loss=  0.2096  Test Acc= \u001b[32m100.0\u001b[0m\n",
            "The Best Epoch based on accuracy: 5\n",
            "\u001b[K ► Test Loss=  0.2149  Test Acc= \u001b[32m100.0\u001b[0m\n",
            "Subject: 45\n",
            "(10600, 1, 62, 250)\n",
            "The Best Epoch: 20\n",
            "\u001b[K ► Test Loss=  0.3344  Test Acc= \u001b[32m96.0\u001b[0m\n",
            "The Best Epoch based on accuracy: 7\n",
            "\u001b[K ► Test Loss=  0.3190  Test Acc= \u001b[32m96.0\u001b[0m\n",
            "Subject: 46\n",
            "(10600, 1, 62, 250)\n",
            "The Best Epoch: 20\n",
            "\u001b[K ► Test Loss=  0.4558  Test Acc= \u001b[32m91.0\u001b[0m\n",
            "The Best Epoch based on accuracy: 7\n",
            "\u001b[K ► Test Loss=  0.4825  Test Acc= \u001b[32m90.0\u001b[0m\n",
            "Subject: 47\n",
            "(10600, 1, 62, 250)\n",
            "The Best Epoch: 20\n",
            "\u001b[K ► Test Loss=  0.5171  Test Acc= \u001b[32m84.0\u001b[0m\n",
            "The Best Epoch based on accuracy: 7\n",
            "\u001b[K ► Test Loss=  0.4976  Test Acc= \u001b[32m84.0\u001b[0m\n",
            "Subject: 48\n",
            "(10600, 1, 62, 250)\n",
            "The Best Epoch: 20\n",
            "\u001b[K ► Test Loss=  682.8708  Test Acc= \u001b[31m48.0\u001b[0m\n",
            "The Best Epoch based on accuracy: 7\n",
            "\u001b[K ► Test Loss=  341.2824  Test Acc= \u001b[31m47.0\u001b[0m\n",
            "Subject: 49\n",
            "(10600, 1, 62, 250)\n",
            "The Best Epoch: 20\n",
            "\u001b[K ► Test Loss=  2.5272  Test Acc= \u001b[33m62.0\u001b[0m\n",
            "The Best Epoch based on accuracy: 8\n",
            "\u001b[K ► Test Loss=  2.4623  Test Acc= \u001b[33m64.0\u001b[0m\n",
            "Subject: 50\n",
            "(10600, 1, 62, 250)\n",
            "The Best Epoch: 20\n",
            "\u001b[K ► Test Loss=  12.2148  Test Acc= \u001b[31m55.0\u001b[0m\n",
            "The Best Epoch based on accuracy: 8\n",
            "\u001b[K ► Test Loss=  25.3808  Test Acc= \u001b[31m55.0\u001b[0m\n",
            "Subject: 51\n",
            "(10600, 1, 62, 250)\n",
            "The Best Epoch: 20\n",
            "\u001b[K ► Test Loss=  0.8259  Test Acc= \u001b[32m82.0\u001b[0m\n",
            "The Best Epoch based on accuracy: 8\n",
            "\u001b[K ► Test Loss=  0.9909  Test Acc= \u001b[32m83.0\u001b[0m\n",
            "Subject: 52\n",
            "(10600, 1, 62, 250)\n",
            "The Best Epoch: 20\n",
            "\u001b[K ► Test Loss=  0.6082  Test Acc= \u001b[32m84.0\u001b[0m\n",
            "The Best Epoch based on accuracy: 8\n",
            "\u001b[K ► Test Loss=  0.6198  Test Acc= \u001b[32m84.0\u001b[0m\n",
            "Subject: 53\n",
            "(10600, 1, 62, 250)\n",
            "The Best Epoch: 20\n",
            "\u001b[K ► Test Loss=  14.7680  Test Acc= \u001b[31m56.0\u001b[0m\n",
            "The Best Epoch based on accuracy: 8\n",
            "\u001b[K ► Test Loss=  12.9817  Test Acc= \u001b[31m55.0\u001b[0m\n",
            "Subject: 54\n",
            "(10600, 1, 62, 250)\n",
            "The Best Epoch: 20\n",
            "\u001b[K ► Test Loss=  5.0167  Test Acc= \u001b[33m66.0\u001b[0m\n",
            "The Best Epoch based on accuracy: 8\n",
            "\u001b[K ► Test Loss=  4.9644  Test Acc= \u001b[33m66.0\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
